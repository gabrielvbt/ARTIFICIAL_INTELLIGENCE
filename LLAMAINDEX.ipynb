{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da57aa8d",
   "metadata": {},
   "source": [
    "#### bibliotecas necessárias\n",
    "- pandas==2.2.2\n",
    "- llama-index==0.11.20\n",
    "- llama-index-llms-groq==0.2.0\n",
    "- llama-index-experimental==0.4.0\n",
    "- gradio==5.4.0\n",
    "- fpdf==1.7.2\n",
    "\n",
    "pip install llama-index==0.11.20 && pip install llama-index-llms-groq==0.2.0 && pip install llama-index-experimental==0.4.0 && pip install gradio==5.4.0 && pip install fpdf==1.7.2\n",
    "\n",
    "- OBS: Também é necessário uma key atravez do https://groq.com, sinalizado na variável \"groq_key\" na celula abaixo.\n",
    "- OBS2: Bem como um arquivo para análise, no caso, para este projeto utilizamos as informações de uma tabela fictícia da Valquíria Alencar, instrutora do curso de LLM LlamaIndex na Alura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058da079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "groq_key = ''\n",
    "url = 'https://github.com/alura-cursos/llamaIndex_pandas_query/blob/main/Dados/vendas.csv'\n",
    "\n",
    "Settings.llm = Groq(model='llama3-70b-8192', api_key=groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bba848",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2944350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A FORMA MAIS DIRETA DE RESPOSTAS UTILIZANDO QUERY_ENGINE\n",
    "query_engine = PandasQueryEngine(df=DF, verbose=True)\n",
    "\n",
    "response = query_engine.query('Qual a forma de pagamento mais utilizada?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb12e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESPOSTAS MAIS COMPLETAS ALÉM DO VALOR FINAL, UMA EXPLICAÇÃO\n",
    "# TEXTWRAP É APENAS PARA FACILITAR A VISUALIZAÇÃO, NÃO É NECESSÁRIO\n",
    "query_engine = PandasQueryEngine(df=DF, verbose=True, synthesize_response=True)\n",
    "\n",
    "response = query_engine.query('Qual a média de avaliação para cada filial?')\n",
    "print(textwrap.fill(response.response, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c18c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# É POSSIVEL FAZER PLOTAGEM DE GRAFICOS, UMA VEZ QUE O PANDAS POSSUI O PLOT\n",
    "query_engine = PandasQueryEngine(df=DF, verbose=True, synthesize_response=True)\n",
    "\n",
    "response = query_engine.query('Você pode plotar a distribuição das avaliações?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc640f1",
   "metadata": {},
   "source": [
    "Quando pedimos uma ação do modelo que não seja uma pergunta, como se fosse uma ordem, o modelo pode responder sem a formatação de linguagem (portugues), e nos trás uma resposta em inglês. Na maioria dos cenários isto é um problema, portanto, vamos fazer os ajustes necessários no prompt\n",
    "\n",
    "query_engine = PandasQueryEngine(df=DF, verbose=True, synthesize_response=True)\n",
    "\n",
    "response = query_engine.query('Plote a distribuição das avaliações')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.experimental.query_engine.pandas import PandasInstructionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO PARA OBTER UMA DESCRIÇÃO DAS COLUNAS DO DATAFRAME\n",
    "def descricao_colunas(DF):\n",
    "    descricao = '\\n'.join([f\"`{col}`: {str(DF[col].dtype)}\" for col in DF.columns])\n",
    "    return 'Aqui estão os detalhes das colunas do DataFrame:\\n' + descricao\n",
    "\n",
    "# INSTRUÇÕES PARA ORIENTAR O MODELO A CONVERTER UMA CONSULTA EM LINGUAGEM NATURAL EM CÓDIGO PYTHON EXECUTÁVEL COM A BIBLIOTECA PANDAS\n",
    "instruction_str = (\"1. Converta a consulta para código Python executável usando Pandas.\\n\"\n",
    "                    \"2. A linha final do código deve ser uma expressão Python que possa ser chamada com a função `eval()`.\\n\"\n",
    "                    \"3. O código deve representar uma solução para a consulta.\\n\"\n",
    "                    \"4. IMPRIMA APENAS A EXPRESSÃO.\\n\"\n",
    "                    \"5. Não coloque a expressão entre aspas.\\n\")\n",
    "\n",
    "# PROMPT QUE INFORMA AO MODELO OS PARAMETROS MAIS BASICOS DAS PERGUNTAS INICIAIS; DATAFRAME, PYTHON, RESULTADO, ETC\n",
    "pandas_prompt_str = (\"Você está trabalhando com um dataframe do pandas em Python chamado `DF`.\\n\"\n",
    "                    \"{colunas_detalhes}\\n\\n\"\n",
    "                    \"Este é o resultado de `print(DF.head())`:\\n\"\n",
    "                    \"{df_str}\\n\\n\"\n",
    "                    \"Siga estas instruções:\\n\"\n",
    "                    \"{instruction_str}\\n\"\n",
    "                    \"Consulta: {query_str}\\n\\n\"\n",
    "                    \"Expressão:\")\n",
    "\n",
    "# GUIA PARA SINTETIZAR A RESPOSTA ATRAVÉS DA PERGUNTA FEITA\n",
    "response_synthesis_prompt_str = (\"Dada uma pergunta de entrada, atue como analista de dados e elabore uma resposta a partir dos resultados da consulta.\\n\"\n",
    "                                \"Responda de forma natural, sem introduções como 'A resposta é:' ou algo semelhante.\\n\"\n",
    "                                \"Consulta: {query_str}\\n\\n\"\n",
    "                                \"Instruções do Pandas (opcional):\\n{pandas_instructions}\\n\\n\"\n",
    "                                \"Saída do Pandas: {pandas_output}\\n\\n\"\n",
    "                                \"Resposta:\"\n",
    "                                \"Ao final, exibir o código usado para gerar a resposta, no formato: O código utilizado foi {pandas_instructions}\")\n",
    "\n",
    "# OBTEM INSTRUÇOES DO PANDAS\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(instruction_str=instruction_str, colunas_detalhes=descricao_colunas(DF), df_str=DF.head(5))\n",
    "# EXECUTA AS INSTRUÇÕES\n",
    "pandas_output_parser = PandasInstructionParser(DF)\n",
    "# SINTETIZA A RESPOSTA\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
    "\n",
    "llm = Groq(model='llama3-70b-8192', api_key=groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e381513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import(QueryPipeline as QP, Link, InputComponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "qp = QP(modules={'input': InputComponent(),\n",
    "                 'pandas_prompt': pandas_prompt,\n",
    "                 'llm1': llm,\n",
    "                 'pandas_output_parser': pandas_output_parser,\n",
    "                 'response_synthesis_prompt': response_synthesis_prompt,\n",
    "                 'llm2': llm}, verbose=True)\n",
    "\n",
    "# DEFININDO A CADEIA DE EVENTOS DO PIPELINE\n",
    "qp.add_chain(['input', 'pandas_prompt', 'llm1', 'pandas_output_parser'])\n",
    "\n",
    "# UNINDO OS PROCESSOS (LINKS)\n",
    "qp.add_links([Link('input', 'response_synthesis_prompt', dest_key='query_str'),\n",
    "              Link('llm1', 'response_synthesis_prompt', dest_key='pandas_instructions'),\n",
    "              Link('pandas_outrput_parser', 'response_synthesis_prompt', dest_key='pandas_output')])\n",
    "\n",
    "qp.add_link('response_synthesis_prompt', 'llm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf274ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qp.run(query_str='Qual é a média gasta por cada tipo de cliente?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c543258",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = response.message.content\n",
    "texto_format = textwrap.fill(texto, width=100)\n",
    "texto_format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
